import streamlit as st
import tensorflow as tf
from tensorflow import keras
import numpy as np
import cv2
import mediapipe as mp
import joblib
from typing import List, Tuple
import matplotlib.pyplot as plt
from datetime import datetime
import pandas as pd
import os
import tempfile
import seaborn as sns
from scipy.stats import pearsonr
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import streamlit as st
import shutil

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="AI å§¿å‹¢è©•ä¼°ç³»çµ±",
    page_icon="ğŸƒâ€â™‚ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å®šç¾©è‡ªè¨‚æå¤±å‡½æ•¸
def rmse(y_true, y_pred):
    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))

# ä½¿ç”¨ os.symlink å‰µå»ºè»Ÿé€£çµä¾†è§£æ±ºæ¬Šé™å•é¡Œ
def setup_mediapipe_model():
    """
    æ­¤å‡½æ•¸æœƒé€éå»ºç«‹è»Ÿé€£çµï¼Œå°‡ mediapipe çš„æ¨¡å‹è·¯å¾‘é‡æ–°å°å‘åˆ°ä¸€å€‹å¯å¯«å…¥çš„ç›®éŒ„ï¼Œ
    ä»¥è§£æ±º Streamlit Cloud ä¸Šçš„ Permission denied éŒ¯èª¤ã€‚
    """
    try:
        # Streamlit Cloud ä¸Šçš„ mediapipe é è¨­æ¨¡å‹ç›®éŒ„
        src_dir = '/home/adminuser/venv/lib/python3.11/site-packages/mediapipe/modules/pose_landmark/'
        
        # æˆ‘å€‘å¯å¯«å…¥çš„æš«å­˜ç›®éŒ„
        dest_dir = '/tmp/mediapipe_pose_models/'
        
        # ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨
        os.makedirs(dest_dir, exist_ok=True)
        
        # æ¨¡å‹æª”æ¡ˆåœ¨ GitHub å„²å­˜åº«çš„æ ¹ç›®éŒ„
        repo_model_path = os.path.join(os.getcwd(), 'pose_landmark_heavy.tflite')
        
        if not os.path.exists(repo_model_path):
            st.error("âŒ æ‰¾ä¸åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸­çš„ 'pose_landmark_heavy.tflite' æª”æ¡ˆï¼Œè«‹ç¢ºä¿å·²ä¸Šå‚³è‡³ GitHubã€‚")
            return False
            
        # å°‡æ¨¡å‹æª”æ¡ˆè¤‡è£½åˆ°æˆ‘å€‘çš„æš«å­˜ç›®éŒ„
        shutil.copyfile(repo_model_path, os.path.join(dest_dir, 'pose_landmark_heavy.tflite'))
        
        # å»ºç«‹ä¸€å€‹è»Ÿé€£çµï¼Œå°‡ mediapipe å°å‘åˆ°æˆ‘å€‘çš„æš«å­˜ç›®éŒ„
        # æ³¨æ„ï¼šæˆ‘å€‘éœ€è¦å…ˆæª¢æŸ¥é€£çµæ˜¯å¦å·²å­˜åœ¨ï¼Œå¦å‰‡æœƒå ±éŒ¯
        if not os.path.exists(src_dir):
            os.makedirs(src_dir, exist_ok=True)
            
        symlink_path = os.path.join(src_dir, 'pose_landmark_heavy.tflite')
        
        if not os.path.exists(symlink_path):
            os.symlink(os.path.join(dest_dir, 'pose_landmark_heavy.tflite'), symlink_path)
        
        st.sidebar.success("âœ… mediapipe æ¨¡å‹è·¯å¾‘å·²æˆåŠŸä¿®å¾©ï¼")
        return True
    
    except Exception as e:
        st.sidebar.error(f"âŒ ä¿®å¾© mediapipe æ¨¡å‹è·¯å¾‘æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False


class PoseEvaluator:
    def __init__(self, model_path: str, scaler_path: str):
        # è¼‰å…¥æ¨¡å‹æ™‚ï¼Œå°‡è‡ªè¨‚å‡½æ•¸å‚³å…¥ custom_objects
        self.model = keras.models.load_model(model_path, custom_objects={'rmse': rmse})
        self.scaler = joblib.load(scaler_path)
        self.mp_pose = mp.solutions.pose
        
        # è¼‰å…¥ mediapipe å§¿å‹¢æ¨¡å‹
        # model_complexity=2 å°æ‡‰ mediapipe çš„ pose_landmark_heavy.tflite
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=2,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

    def process_frame(self, frame: np.ndarray) -> List[float]:
        """è™•ç†å–®å€‹å½±æ ¼ä¸¦æå–é—œéµé»ï¼ˆä½¿ç”¨ç›¸å°åº§æ¨™ï¼‰"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.pose.process(rgb_frame)

        if results.pose_landmarks:
            landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark])

            # ç²å–å·¦é«– (23) å’Œå³é«– (24) çš„åº§æ¨™
            left_hip = landmarks[23, :3]
            right_hip = landmarks[24, :3]
            hip_center = (left_hip + right_hip) / 2

            # ç²å–å·¦è‚© (11) å’Œå³è‚© (12) çš„åº§æ¨™
            left_shoulder = landmarks[11, :3]
            right_shoulder = landmarks[12, :3]
            shoulder_center = (left_shoulder + right_shoulder) / 2

            # è¨ˆç®—æ¨™æº–åŒ–å°ºåº¦
            scale = np.linalg.norm(shoulder_center - hip_center)
            if scale == 0:
                scale = 1

            # å°‡æ‰€æœ‰é—œéµé»è½‰æ›ç‚ºç›¸å°æ–¼é«–é—œç¯€ä¸­å¿ƒçš„åº§æ¨™ï¼Œä¸¦é€²è¡Œå°ºåº¦æ¨™æº–åŒ–
            normalized_landmarks = (landmarks[:, :3] - hip_center) / scale
            normalized_landmarks = np.hstack((normalized_landmarks, landmarks[:, 3:4]))

            return normalized_landmarks.flatten().tolist()
        return None

    def analyze_predictions(self, predictions: np.ndarray) -> dict:
        """åˆ†æé æ¸¬çµæœä¸¦è¿”å›è©³ç´°çµ±è¨ˆè³‡è¨Š"""
        predictions = predictions.flatten()

        return {
            'min': np.min(predictions),
            'max': np.max(predictions),
            'mean': np.mean(predictions),
            'median': np.median(predictions),
            'std': np.std(predictions),
            '25th_percentile': np.percentile(predictions, 25),
            '75th_percentile': np.percentile(predictions, 75)
        }

    def evaluate_video(self, video_path: str, progress_callback=None) -> Tuple[float, dict, np.ndarray]:
        """è©•ä¼°å½±ç‰‡ä¸­çš„å‹•ä½œä¸¦è¿”å›è©³ç´°åˆ†æ"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        keypoints_list = []
        frame_count = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            if progress_callback:
                progress_callback(frame_count / total_frames)

            keypoints = self.process_frame(frame)
            if keypoints:
                keypoints_list.append(keypoints)

        cap.release()

        if len(keypoints_list) == 0:
            return None, None, None

        # æ•¸æ“šé è™•ç†
        keypoints_array = np.array(keypoints_list)
        keypoints_array = self.scaler.transform(keypoints_array)
        keypoints_array = keypoints_array.reshape(-1, 33, 4)

        # é€²è¡Œé æ¸¬
        predictions = self.model.predict(keypoints_array, batch_size=32, verbose=0)
        stats = self.analyze_predictions(predictions)

        # æ·»åŠ é¡å¤–çµ±è¨ˆè³‡è¨Š
        stats.update({
            'total_frames': total_frames,
            'effective_frames': len(keypoints_list),
            'fps': fps,
            'duration': total_frames/fps,
            'detection_rate': (len(keypoints_list)/total_frames)*100
        })

        return stats['mean'], stats, predictions

def create_interactive_plots(predictions):
    """å‰µå»ºäº’å‹•å¼åœ–è¡¨"""
    predictions_flat = predictions.flatten()

    # å‰µå»ºå­åœ–
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('é æ¸¬å€¼éš¨æ™‚é–“è®ŠåŒ–', 'é æ¸¬å€¼åˆ†å¸ƒ'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}]]
    )

    # 1. é æ¸¬å€¼éš¨æ™‚é–“è®ŠåŒ–
    fig.add_trace(
        go.Scatter(y=predictions_flat, mode='lines', name='é æ¸¬å€¼', line=dict(color='blue')),
        row=1, col=1
    )
    fig.add_hline(y=np.mean(predictions_flat), line_dash="dash", line_color="red",
                  annotation_text="å¹³å‡å€¼", row=1, col=1)

    # 2. é æ¸¬å€¼åˆ†å¸ƒç›´æ–¹åœ–
    fig.add_trace(
        go.Histogram(x=predictions_flat, name='åˆ†å¸ƒ', nbinsx=30, opacity=0.7),
        row=1, col=2
    )

    fig.update_layout(height=400, showlegend=True, title_text="å§¿å‹¢è©•ä¼°åˆ†æçµæœ")
    return fig

def main():
    st.title("ğŸƒâ€â™‚ï¸ AI å§¿å‹¢è©•ä¼°ç³»çµ±")
    st.markdown("---")

    # åœ¨è¼‰å…¥æ¨¡å‹å‰å…ˆé€²è¡Œ mediapipe çš„è·¯å¾‘ä¿®å¾©
    if not setup_mediapipe_model():
        st.sidebar.error("âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•ä¿®å¾© mediapipe æ¨¡å‹è·¯å¾‘ã€‚")
        st.stop()
        
    # å´é‚Šæ¬„ - è¨­å®šåƒæ•¸
    st.sidebar.header("âš™ï¸ ç³»çµ±è¨­å®š")

    # æ¨¡å‹æª”æ¡ˆè·¯å¾‘è¨­å®š
    model_path = st.sidebar.text_input(
        "æ¨¡å‹æª”æ¡ˆè·¯å¾‘",
        value="CNN_squat_best.keras",
        help="è«‹è¼¸å…¥è¨“ç·´å¥½çš„ Keras æ¨¡å‹æª”æ¡ˆè·¯å¾‘"
    )

    scaler_path = st.sidebar.text_input(
        "æ¨™æº–åŒ–å™¨æª”æ¡ˆè·¯å¾‘",
        value="scaler_CNN_squat_best.pkl",
        help="è«‹è¼¸å…¥ç”¨æ–¼è³‡æ–™æ¨™æº–åŒ–çš„ scaler æª”æ¡ˆè·¯å¾‘"
    )
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    files_exist = all([
        os.path.exists(model_path) if model_path else False,
        os.path.exists(scaler_path) if scaler_path else False
    ])

    if not files_exist:
        st.error("âŒ è«‹ç¢ºèªæ¨¡å‹æª”æ¡ˆå’Œæ¨™æº–åŒ–å™¨æª”æ¡ˆè·¯å¾‘æ­£ç¢º")
        st.stop()

    # åˆå§‹åŒ–è©•ä¼°å™¨
    try:
        with st.spinner("æ­£åœ¨è¼‰å…¥æ¨¡å‹..."):
            evaluator = PoseEvaluator(model_path, scaler_path)
        st.sidebar.success("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        st.sidebar.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
        st.stop()

    # ä¸»è¦å…§å®¹å€åŸŸ
    col1, col2 = st.columns([2, 1])

    with col1:
        st.header("ğŸ“¹ å½±ç‰‡ä¸Šå‚³èˆ‡åˆ†æ")

        # å½±ç‰‡ä¸Šå‚³
        uploaded_file = st.file_uploader(
            "é¸æ“‡è¦åˆ†æçš„å½±ç‰‡æª”æ¡ˆ",
            type=['mp4', 'avi', 'mov', 'mkv'],
            help="æ”¯æ´æ ¼å¼ï¼šMP4, AVI, MOV, MKV"
        )

        if uploaded_file is not None:
            # å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆåˆ°è‡¨æ™‚ä½ç½®
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                tmp_file.write(uploaded_file.read())
                temp_video_path = tmp_file.name

            st.success(f"âœ… å½±ç‰‡å·²ä¸Šå‚³: {uploaded_file.name}")

            # é¡¯ç¤ºå½±ç‰‡è³‡è¨Š
            cap = cv2.VideoCapture(temp_video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            cap.release()

            st.info(f"ğŸ“Š å½±ç‰‡è³‡è¨Š: {duration:.2f}ç§’ | {total_frames}å¹€ | {fps:.1f} FPS | {width}x{height}")

            # åˆ†ææŒ‰éˆ•
            if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
                progress_bar = st.progress(0)
                status_text = st.empty()

                def update_progress(progress):
                    progress_bar.progress(progress)
                    status_text.text(f"åˆ†æé€²åº¦: {progress*100:.1f}%")

                try:
                    # åŸ·è¡Œåˆ†æ
                    avg_score, detailed_stats, predictions = evaluator.evaluate_video(
                        temp_video_path,
                        progress_callback=update_progress
                    )

                    if avg_score is not None:
                        status_text.text("âœ… åˆ†æå®Œæˆï¼")

                        # å„²å­˜çµæœåˆ° session state
                        st.session_state['analysis_results'] = {
                            'avg_score': avg_score,
                            'detailed_stats': detailed_stats,
                            'predictions': predictions,
                            'video_name': uploaded_file.name
                        }
                    else:
                        st.error("âŒ æœªæª¢æ¸¬åˆ°æœ‰æ•ˆçš„å§¿å‹¢æ•¸æ“šï¼Œè«‹æª¢æŸ¥å½±ç‰‡å“è³ª")

                except Exception as e:
                    st.error(f"âŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

                finally:
                    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                    if os.path.exists(temp_video_path):
                        os.unlink(temp_video_path)

    with col2:
        st.header("ğŸ“ˆ å³æ™‚çµ±è¨ˆ")

        if 'analysis_results' in st.session_state:
            results = st.session_state['analysis_results']
            stats = results['detailed_stats']

            # é¡¯ç¤ºä¸»è¦åˆ†æ•¸
            st.metric(
                label="å¹³å‡å§¿å‹¢åˆ†æ•¸",
                value=f"{results['avg_score']:.2f}",
                delta=f"Â±{stats['std']:.2f}"
            )

            # é¡¯ç¤ºå…¶ä»–çµ±è¨ˆè³‡è¨Š
            col2_1, col2_2 = st.columns(2)
            with col2_1:
                st.metric("æœ€é«˜åˆ†", f"{stats['max']:.2f}")
                st.metric("æª¢æ¸¬ç‡", f"{stats['detection_rate']:.1f}%")
            with col2_2:
                st.metric("æœ€ä½åˆ†", f"{stats['min']:.2f}")
                st.metric("å½±ç‰‡é•·åº¦", f"{stats['duration']:.1f}ç§’")

    # çµæœè¦–è¦ºåŒ–å€åŸŸ
    if 'analysis_results' in st.session_state:
        st.markdown("---")
        st.header("ğŸ“Š è©³ç´°åˆ†æçµæœ")

        results = st.session_state['analysis_results']
        predictions = results['predictions']

        # å‰µå»ºäº’å‹•å¼åœ–è¡¨
        fig = create_interactive_plots(predictions)
        st.plotly_chart(fig, use_container_width=True)

        # è©³ç´°çµ±è¨ˆè¡¨æ ¼
        st.subheader("ğŸ“‹ çµ±è¨ˆæ‘˜è¦")
        stats_df = pd.DataFrame([
            ['å½±ç‰‡åç¨±', results['video_name']],
            ['å¹³å‡åˆ†æ•¸', f"{results['avg_score']:.2f}"],
            ['æ¨™æº–å·®', f"{results['detailed_stats']['std']:.2f}"],
            ['æœ€å°å€¼', f"{results['detailed_stats']['min']:.2f}"],
            ['æœ€å¤§å€¼', f"{results['detailed_stats']['max']:.2f}"],
            ['ä¸­ä½æ•¸', f"{results['detailed_stats']['median']:.2f}"],
            ['25th ç™¾åˆ†ä½', f"{results['detailed_stats']['25th_percentile']:.2f}"],
            ['75th ç™¾åˆ†ä½', f"{results['detailed_stats']['75th_percentile']:.2f}"],
            ['ç¸½å¹€æ•¸', f"{results['detailed_stats']['total_frames']}"],
            ['æœ‰æ•ˆå¹€æ•¸', f"{results['detailed_stats']['effective_frames']}"],
            ['æª¢æ¸¬ç‡', f"{results['detailed_stats']['detection_rate']:.1f}%"],
            ['å½±ç‰‡é•·åº¦', f"{results['detailed_stats']['duration']:.2f} ç§’"]
        ], columns=['é …ç›®', 'æ•¸å€¼'])

        st.dataframe(stats_df, use_container_width=True)
            
if __name__ == "__main__":
    main()